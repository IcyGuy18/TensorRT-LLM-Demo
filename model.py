from tensorrt_llm.runtime.generation import (
    GenerationSession, SamplingConfig
)
from tensorrt_llm.runtime.model_runner import (
    ModelRunner, ModelConfig
)
from tensorrt_llm.builder import (
    Engine, EngineConfig
)

from transformers import (
    AutoTokenizer, PreTrainedTokenizer
)

import torch

import typing
import tqdm
import os

class Model:
    """The Model class handles engines built using TensorRT-LLM.
    
    Methods
    -------
    generate(prompt, stream, **kwargs):
        Returns response generated by the LLM based on prompt and arguments.
    """
    def __init__(
        self,
        model_dir: typing.AnyStr
    ) -> None:
        # Initialise the tokenizer and engine
        self._tokenizer = AutoTokenizer.from_pretrained(
            model_dir
        )
        #? Safety check.
        if self._tokenizer.pad_token_id is None:
            self._tokenizer.pad_token_id = self._tokenizer.eos_token_id

        self._model = self.__retrieve_engine(model_dir + "_TRT")

    def __retrieve_engine(
        self,
        engine_dir: typing.AnyStr
    ) -> ModelRunner:
        ## Load the engine into memory.
        #engine = Engine.from_dir(engine_dir=engine_dir, rank=0)

        rank = 0
        #? Ignore the rank settings - that's useful for multi-process
        #? communication (for a later time).
        # We'll fetch the file size for loading the engine.
        file_size = os.path.getsize(os.path.join(engine_dir, f'rank{rank}.engine'))
        # Then loading the engine using TQDM for progress (Neatness!!!)
        with open(os.path.join(engine_dir, f'rank{rank}.engine'), 'rb') as f:
            with tqdm.tqdm(
                total=file_size,
                desc='Loading model...',
                unit='B',
                unit_scale=True,
            ) as pbar:
                engine_buffer = bytearray()
                chunk_size = 32768
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    engine_buffer += chunk
                    pbar.update(len(chunk))

        config = EngineConfig.from_json_file(
            os.path.join(engine_dir, 'config.json'))
        config.pretrained_config.set_rank(rank)

        engine = Engine(config, engine_buffer)

        ## Load in the pretrained and build configurations given in the JSON.
        pretrained_config = engine.config.pretrained_config
        build_config = engine.config.build_config

        ## Calculate num_heads, num_kv_heads, hidden_size, and head_size.
        tp_size = pretrained_config.mapping.tp_size
        num_heads = pretrained_config.num_attention_heads // tp_size
        num_kv_heads = (
            pretrained_config.num_key_value_heads + tp_size - 1
        ) // tp_size
        hidden_size = pretrained_config.hidden_size // tp_size
        head_size = pretrained_config.head_size

        ## Set up a ModelConfig attribute.
        #* You can mess around with the class attribute as needed,
        #* but it's advised not to.
        model_config = ModelConfig(
            max_batch_size=build_config.max_batch_size,
            max_beam_width=build_config.max_beam_width,
            vocab_size=pretrained_config.vocab_size,
            num_layers=pretrained_config.num_hidden_layers,
            num_heads=num_heads,
            num_kv_heads=num_kv_heads,
            hidden_size=hidden_size,
            head_size=head_size,
            gpt_attention_plugin=bool(
                build_config.plugin_config.gpt_attention_plugin),
            mamba_conv1d_plugin=bool(
                build_config.plugin_config.mamba_conv1d_plugin),
            remove_input_padding=build_config.plugin_config.
            remove_input_padding,
            paged_kv_cache=build_config.plugin_config.paged_kv_cache,
            paged_state=build_config.plugin_config.paged_state,
            tokens_per_block=build_config.plugin_config.tokens_per_block,
            quant_mode=pretrained_config.quant_mode,
            gather_context_logits=build_config.gather_context_logits,
            gather_generation_logits=build_config.gather_generation_logits,
            dtype=pretrained_config.dtype,
            max_prompt_embedding_table_size=build_config.
            max_prompt_embedding_table_size,
            lora_plugin=build_config.plugin_config.lora_plugin,
            lora_target_modules=build_config.lora_config.lora_target_modules,
            trtllm_modules_to_hf_modules=build_config.lora_config.
            trtllm_modules_to_hf_modules,
            max_medusa_tokens=pretrained_config.max_draft_len if hasattr(
                pretrained_config, 'max_draft_len') else 0,
            num_medusa_heads=pretrained_config.num_medusa_heads if hasattr(
                pretrained_config, 'num_medusa_heads') else 0,
            use_custom_all_reduce=build_config.plugin_config.
            use_custom_all_reduce,
        )

        # Set up a GenerationSession attribute
        session = GenerationSession(
            model_config, # Provide model configuration in here
            engine.engine, # Load the engine buffer here
            pretrained_config.mapping, # Load the pretrained configuration
            False, # Debug Mode, not necessary to mess with
            None #! Stream - KEEP THIS AS IS IF YOU DON'T KNOW WHAT THIS DOES
        )
        
        # Finally, create a ModelRunner class and return it
        model_runner = ModelRunner(
            session=session, # The GenerationSession object
            max_batch_size=engine.config.build_config.max_batch_size, # Match batch size
            max_input_len=engine.config.build_config.max_input_len, # Mac input length
            max_seq_len=( # The max sequence length (both input and output combined)
                engine.config.build_config.max_input_len + 
                engine.config.build_config.max_output_len
            ),
            max_beam_width=engine.config.build_config.max_beam_width, # Max beam width
            lora_manager=None # LoRA Manager - not needed unless doing something with LoRA
        )
        return model_runner

    def generate( # NOSONAR
        self,
        prompt: typing.AnyStr |
                typing.List[typing.Dict[typing.Text, typing.Text]],
        stream: bool = False,
        **kwargs: typing.Dict
    ) -> str | typing.Generator:
        """Returns response generated by the LLM based on the prompt and
        parameters.
        
        Parameters
        ----------
        prompt : string | list[dict[str, str]]
            The input prompt against which the LLM generates a response.
            
            Note that it is preferred to use the list-of-dictionaries method
            for model-agnostic compatibility. The routine handles roles and
            contents by itself.

            If string is passed, it is assumed that the requester already
            applied the tokenizer chat template from their end.
            
            Accepts string or list-of-dictionary values.
        
        stream : bool (default: False)
            Enabling it allows the response to be presented as soon as a token
            is generated.
            
            By default, it is disabled, which returns the response once the
            LLM has generated all tokens.
            
            Make sure that the return types are handled accordingly at your
            end when enabling/disabling the streaming option.
            
            Accepts boolean values.
        
        kwargs : dict
            max_new_tokens : int (default: 8192)
                How many new tokens at max should the LLM generate in the
                response.

                Accepts strictly positive values.

            do_sample : bool (default: True)
                Perform probabilistic sampling to enable different responses
                and variations by the LLM. If false, uses greedy decoding.
                
                Accepts boolean values.

            temperature : float (default: 1.0)
                Used to modulate next token probabilities. Values closer to 0
                results in more deterministic responses, while values closer
                to 1 result in, for a lack of a better word, 'creative'
                responses.
                
                Accepts values between 0 and 1 inclusive.

            top_p : float (default: 1.0)
                If set less than 1, picks the smallest set of most probable
                tokens that add up to top_p or higher for generation.

                Accepts values between 0 and 1 inclusive.

            top_k : int (default: 50)
                - The number of highest probability vocabulary tokens to keep
                for top-k filtering.

                Accepts positive values only.

            repetition_penalty : float (default: 1.0)
                How much penalty is applied to the LLM for repeating tokens.

                Accepts positive values greater than or equal to 1.0

        Returns
        -------
        str
            The response generated by the LLM engine when streaming is
            disabled.
        
        Yields
        ------
        str
            The response generated by the LLM engine when streaming is
            enabled. Note that each generated token is immediately yielded.
        
        Raises
        ------
        TypeError
            An invalid datatype is sent to the routine.
        ValueError
            A value that does not conform to the defined ranges.
        """

        def _check_inputs(**kwargs) -> tuple[str|list, bool, dict]:
            # Check if keyword arguments exist. If not, set defaults.
            if not kwargs.get('max_new_tokens'):
                kwargs.update({'max_new_tokens': 8192})
            if not kwargs.get('do_sample'):
                kwargs.update({'do_sample': True})
            if not kwargs.get('temperature'):
                kwargs.update({'temperature': 1.0})
            if not kwargs.get('top_p'):
                kwargs.update({'top_p': 1.0})
            if not kwargs.get('top_k'):
                kwargs.update({'top_k': 50})
            if not kwargs.get('repetition_penalty'):
                kwargs.update({'repetition_penalty': 1.0})
            
            # Check if inputs conform to the datatypes
            if not isinstance(kwargs.get('max_new_tokens'), int):
                raise TypeError("'max_new_tokens' is not of integer type.")
            if not isinstance(kwargs.get('do_sample'), bool):
                raise TypeError("'do_sample' is not of boolean type.")
            if not isinstance(kwargs.get('temperature'), float):
                raise TypeError("'temperature' is not of float type.")
            if not isinstance(kwargs.get('top_p'), float):
                raise TypeError("'top_p' is not of float type.")
            if not isinstance(kwargs.get('top_k'), int):
                raise TypeError("'top_k' is not of integer type.")
            if not isinstance(kwargs.get('repetition_penalty'), float):
                raise TypeError("'repetition_penalty' is not of float type.")
            if not isinstance(kwargs.get('stream'), bool):
                raise TypeError("'stream' is not of boolean type.")
            if (
                not isinstance(kwargs.get('prompt'), str)
                and
                not isinstance(kwargs.get('prompt'), list)
            ):
                raise TypeError(
                    "The input prompt does not "
                    "conform to the required datatypes."
                )

            # Finally, check if certain inputs conform to the ranges
            if kwargs.get('max_new_tokens') < 1:
                raise ValueError("'max_new_tokens' is not positive.")
            if (
                kwargs.get('temperature') > 1.0
                or
                kwargs.get('temperature') < 0.0
            ):
                raise ValueError("'temperature' is not between 0 and 1.")
            if kwargs.get('top_p') > 1.0 or kwargs.get('top_p') < 0.0:
                raise ValueError("'top_p' is not between 0 and 1.")
            if kwargs.get('repetition_penalty') < 1.0:
                raise ValueError("'repetition_penalty' is less than 1.")

            return kwargs.pop('stream'), kwargs
        
        #*  This function is necessary to format the response.
        #? I have no idea why the responses are so weird to handle in TRT-LLM.
        #! Do NOT attempt to understand this function alone unless you are
        #! confident that no code scares you.
        def _format_response(
            output: list[torch.Tensor],
            tokenizer: PreTrainedTokenizer,
            input_length: int,
            previous_output: str | None = None
        ) -> str:
            return (
            ''.join(
                tokenizer.decode(
                    output[input_length+1:],
                    skip_special_tokens=True,
                ).split('assistant')[1:]
            ).strip()

            if previous_output is None else
            #* Oh my God, this one is worse. Do not attempt to change this.
            #! Your code WILL depend on it, one way or another.
            
            ''.join(
                tokenizer.decode(
                    output[input_length:],
                    skip_special_tokens=True,
                ).split('assistant')[1:]
            ).strip()[len(previous_output):])


        #* Here are the steps followed:
        #  1. The parameters are validated for response.
        #  2. If the prompt is in a list-of-dictionary format, the
        #     appropriate chat template is applied. Otherwise, skipped.
        #     The prompt is then converted into a list of Tensors for the LLM.
        #  3. The engine is given the Tensors along with parameters.
        #  4. The response is returned.

        #! 1
        print(prompt)
        stream, kwargs = _check_inputs(
            prompt=prompt,
            stream=stream,
            **kwargs
        )

        #! 2
        if isinstance(text, list):
            # Apply chat template
            try:
                text = self._tokenizer.apply_chat_template(
                    text,
                    tokenize=False,
                    add_generation_prompt=True,
                )
            except: # NOSONAR I am too lazy to figure out what the exception is.
                # There must be a system role which is not supported by the model's tokenizer
                if len(text) == 1:
                    text[0]['role'] = 'user'
                else:
                    system_prompt = text[0]['content']
                    text.pop(0)
                    text[0]['content'] = system_prompt + "\n\n" + text[0]['content']
                    # Now apply the chat template
                text = self._tokenizer.apply_chat_template(
                    text,
                    tokenize=False,
                    add_generation_prompt=True,
                )
        else:
            input_ids = self._tokenizer(
                prompt,
                return_tensors='pt',
            )
            input_length = input_ids.shape[1]
        
        #! 3
        print(input_ids)
        #? For your sanity, please do not consult this piece of code alone.
        if not stream:
            #! 4
            output = _format_response(
                self._model.generate(
                    input_ids,
                    end_id=self._tokenizer.eos_token_id,
                    pad_id=self._tokenizer.pad_token_id,
                    max_new_tokens=kwargs.get('max_new_tokens'),
                    temperature=kwargs.get('temperature'),
                    top_p=kwargs.get('top_p'),
                    top_k=kwargs.get('top_k'),
                    repetition_penalty=kwargs.get('repetition_penalty'),
                    do_sample=kwargs.get('do_sample'),
                    sampling_config=None,
                )[0][0],
                self._tokenizer,
                input_length
            )
            yield output
        else:
            previous_output = ''
            for output in self._model.generate(
                input_ids,
                streaming=True,
                end_id=self._tokenizer.eos_token_id,
                pad_id=self._tokenizer.pad_token_id,
                max_new_tokens=kwargs.get('max_new_tokens'),
                temperature=kwargs.get('temperature'),
                top_p=kwargs.get('top_p'),
                top_k=kwargs.get('top_k'),
                repetition_penalty=kwargs.get('repetition_penalty'),
                do_sample=kwargs.get('do_sample'),
                sampling_config=None,
            ):
                #! also 4
                generated_response = _format_response(
                    output[0][0],
                    self._tokenizer,
                    input_length,
                    previous_output
                )
                previous_output += generated_response
                yield generated_response